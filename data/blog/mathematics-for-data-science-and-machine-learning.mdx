---
title: 'Mathematics for Data Science & Machine Learning'
date: '2025-12-25'
lastmod: '2025-12-25'
tags:
  [
    'mathematics',
    'data-science',
    'machine-learning',
    'linear-algebra',
    'calculus',
    'statistics',
    'probability',
    'neural-networks',
    'deep-learning',
  ]
draft: false
summary: 'A comprehensive reference for essential mathematical formulas and concepts used in Data Science and Machine Learning â€” covering linear algebra, calculus, probability, statistics, regression, neural networks, optimization, and more.'
authors: ['default']
---

import Twemoji from './components/ui/Twemoji';

## Introduction

Mathematics is the backbone of Data Science and Machine Learning. This comprehensive guide covers the essential formulas, definitions, and concepts you need to master to excel in the field.

---

## 1. Linear Algebra

### Vectors and Matrices

**Dot Product**

$$\mathbf{a} \cdot \mathbf{b} = \sum_{i=1}^{n} a_i b_i$$

**Matrix Multiplication**

$$(AB)_{ij} = \sum_{k=1}^{n} A_{ik} B_{kj}$$

**Transpose**

$$(AB)^T = B^T A^T$$

**Inverse**

$$A A^{-1} = A^{-1} A = I$$

### Norms

| Norm           | Formula                                   |
| -------------- | ----------------------------------------- |
| L1 (Manhattan) | $\|x\|_1 = \sum_i \|x_i\|$                |
| L2 (Euclidean) | $\|x\|_2 = \sqrt{\sum_i x_i^2}$           |
| Frobenius      | $\|A\|_F = \sqrt{\sum_i \sum_j a_{ij}^2}$ |

### Eigenvalues and Eigenvectors

- **Eigen equation:** $Av = \lambda v$
- **Characteristic equation:** $\det(A - \lambda I) = 0$
- **Trace:** $\mathrm{tr}(A) = \sum_i \lambda_i$
- **Determinant:** $\det(A) = \prod_i \lambda_i$

---

## 2. Calculus

### Derivatives and Gradients

| Rule       | Formula                                                                                                 |
| ---------- | ------------------------------------------------------------------------------------------------------- |
| Derivative | $f'(x) = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h}$                                                        |
| Gradient   | $\nabla f(x) = \left( \frac{\partial f}{\partial x_1}, \ldots, \frac{\partial f}{\partial x_n} \right)$ |
| Hessian    | $H_{ij} = \frac{\partial^2 f}{\partial x_i \partial x_j}$                                               |

### Activation Derivatives

| Function | Derivative                                 |
| -------- | ------------------------------------------ |
| Sigmoid  | $\sigma'(x) = \sigma(x)(1 - \sigma(x))$    |
| Tanh     | $\tanh'(x) = 1 - \tanh^2(x)$               |
| ReLU     | $\text{ReLU}'(x) = 1$ if $x > 0$, else $0$ |

---

## 3. Probability

### Basics

**Probability**

$$P(A) = \frac{\text{favorable outcomes}}{\text{total outcomes}}$$

**Complement**

$$P(A^c) = 1 - P(A)$$

**Addition Rule**

$$P(A \cup B) = P(A) + P(B) - P(A \cap B)$$

**Conditional Probability**

$$P(A|B) = \frac{P(A \cap B)}{P(B)}$$

### Bayes Theorem

$$P(A|B) = \frac{P(B|A) P(A)}{P(B)}$$

### Expectation and Variance

| Concept                  | Formula                                       |
| ------------------------ | --------------------------------------------- |
| Expectation (Discrete)   | $E[X] = \sum x_i P(x_i)$                      |
| Expectation (Continuous) | $E[X] = \int x f(x) dx$                       |
| Variance                 | $\text{Var}(X) = E[X^2] - (E[X])^2$           |
| Covariance               | $\text{Cov}(X,Y) = E[(X - \mu_X)(Y - \mu_Y)]$ |

---

## 4. Statistics

### Descriptive Statistics

**Mean**

$$\mu = \frac{1}{n} \sum_{i=1}^{n} x_i$$

**Sample Variance**

$$s^2 = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})^2$$

**Population Variance**

$$\sigma^2 = \frac{1}{n} \sum_{i=1}^{n} (x_i - \mu)^2$$

### Hypothesis Testing

**Z-score**

$$z = \frac{x - \mu}{\sigma}$$

**T-statistic**

$$t = \frac{\bar{x} - \mu}{s / \sqrt{n}}$$

**Confidence Interval**

$$CI = \bar{x} \pm z_{\alpha/2} \frac{\sigma}{\sqrt{n}}$$

---

## 5. Regression

### Linear Regression

**Model**

$$y = \beta_0 + \beta_1 x + \varepsilon$$

**Matrix Form**

$$\mathbf{y} = \mathbf{X} \beta + \varepsilon$$

**Normal Equation**

$$\beta = (X^T X)^{-1} X^T y$$

**Mean Squared Error (MSE)**

$$MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2$$

**R-squared**

$$R^2 = 1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2}$$

### Logistic Regression

**Sigmoid Function**

$$\sigma(z) = \frac{1}{1 + e^{-z}}$$

**Log Loss (Binary Cross-Entropy)**

$$L = -\frac{1}{n} \sum_{i=1}^{n} [y_i \log \hat{y}_i + (1 - y_i) \log(1 - \hat{y}_i)]$$

### Regularization

| Type        | Cost Function                                                                                 |
| ----------- | --------------------------------------------------------------------------------------------- |
| Ridge (L2)  | $J(\beta) = \sum (y_i - \hat{y}_i)^2 + \lambda \sum \beta_j^2$                                |
| Lasso (L1)  | $J(\beta) = \sum (y_i - \hat{y}_i)^2 + \lambda \sum \|\beta_j\|$                              |
| Elastic Net | $J(\beta) = \sum (y_i - \hat{y}_i)^2 + \lambda_1 \sum \|\beta_j\| + \lambda_2 \sum \beta_j^2$ |

---

## 6. Neural Networks

### Activation Functions

| Function | Formula                                                |
| -------- | ------------------------------------------------------ |
| Sigmoid  | $\sigma(x) = \frac{1}{1 + e^{-x}}$                     |
| Tanh     | $\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$         |
| ReLU     | $\text{ReLU}(x) = \max(0, x)$                          |
| Softmax  | $\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}}$ |

### Forward and Backpropagation

**Linear Transformation**

$$z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}$$

**Activation**

$$a^{(l)} = g(z^{(l)})$$

**Output Error**

$$\delta^{(L)} = (a^{(L)} - y) \odot g'(z^{(L)})$$

**Hidden Error**

$$\delta^{(l)} = [(W^{(l+1)})^T \delta^{(l+1)}] \odot g'(z^{(l)})$$

**Weight Gradient**

$$\frac{\partial L}{\partial W^{(l)}} = \delta^{(l)} (a^{(l-1)})^T$$

---

## 7. Optimization

### Gradient Descent

**Update Rule**

$$\theta := \theta - \alpha \nabla J(\theta)$$

> **Note:** $\alpha$ is the learning rate.

### Advanced Optimizers

**Momentum**

$$v := \beta v + (1 - \beta) \nabla J(\theta)$$

$$\theta := \theta - \alpha v$$

**Adam (Simplified)**

$$m := \beta_1 m + (1 - \beta_1) \nabla J$$

$$v := \beta_2 v + (1 - \beta_2) (\nabla J)^2$$

$$\theta := \theta - \alpha \frac{\hat{m}}{\sqrt{\hat{v}} + \epsilon}$$

> **Pro tip:** Adam is the most widely used optimizer for deep learning training.

---

## 8. Evaluation Metrics

### Classification Metrics

| Metric    | Formula                                                                                 |
| --------- | --------------------------------------------------------------------------------------- |
| Accuracy  | $\frac{TP + TN}{TP + TN + FP + FN}$                                                     |
| Precision | $\frac{TP}{TP + FP}$                                                                    |
| Recall    | $\frac{TP}{TP + FN}$                                                                    |
| F1-score  | $\frac{2 \cdot \text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}$ |

### ROC and AUC

**True Positive Rate (TPR)**

$$TPR = \frac{TP}{TP + FN}$$

**False Positive Rate (FPR)**

$$FPR = \frac{FP}{FP + TN}$$

---

## 9. Clustering

### K-Means

**Objective Function**

$$\min \sum_{j=1}^{k} \sum_{x \in C_j} \|x - \mu_j\|^2$$

**Centroid Update**

$$\mu_j = \frac{1}{|C_j|} \sum_{x \in C_j} x$$

### Distance Metrics

| Metric            | Formula                                        |
| ----------------- | ---------------------------------------------- |
| Euclidean         | $d(x,y) = \sqrt{\sum_i (x_i - y_i)^2}$         |
| Manhattan         | $d(x,y) = \sum_i \|x_i - y_i\|$                |
| Cosine Similarity | $\cos(\theta) = \frac{x \cdot y}{\|x\| \|y\|}$ |

---

## 10. Deep Learning

### CNN, RNN, LSTM

**Convolutional Layer Output Size**

$$O = \left\lfloor \frac{W - K + 2P}{S} \right\rfloor + 1$$

Where:

- $W$ = Input size
- $K$ = Kernel size
- $P$ = Padding
- $S$ = Stride

**LSTM Forget Gate**

$$f_t = \sigma(W_f [h_{t-1}, x_t] + b_f)$$

### Batch Normalization and Dropout

**Batch Normalization**

$$\hat{x} = \frac{x - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}$$

**Scale and Shift**

$$y = \gamma \hat{x} + \beta$$

---

## Bonus Topics

### Principal Component Analysis (PCA)

**Covariance Matrix**

$$\Sigma = \frac{1}{n} X^T X$$

**Explained Variance Ratio**

$$\text{EVR} = \frac{\lambda_i}{\sum_j \lambda_j}$$

### Bias-Variance Tradeoff

**Error Decomposition**

$$E[(y - \hat{y})^2] = \text{Bias}^2 + \text{Variance} + \text{Irreducible Error}$$

---

## Conclusion

This comprehensive guide covers the fundamental mathematical concepts essential for Data Science and Machine Learning. From linear algebra to deep learning, these formulas form the building blocks for understanding and implementing ML algorithms.

Happy learning! <Twemoji emoji="clinking-beer-mugs" />
